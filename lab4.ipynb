{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 10 --executor-memory 5g --executor-cores 4 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f682edac278>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2022-01-06 18:46 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', sep='\\t', header=True)\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41138"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "|     -|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select(\"gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.filter(train_data.gender=='-').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  age|\n",
      "+-----+\n",
      "| >=55|\n",
      "|45-54|\n",
      "|    -|\n",
      "|35-44|\n",
      "|25-34|\n",
      "|18-24|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select(\"age\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36138"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.filter((train_data.gender != '-') & (train_data.age != '-'))\n",
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {'F': '0', 'M': '1'}\n",
    "age_dict = {'18-24': '0', '25-34': '1', '35-44': '2', '45-54': '3', '>=55': '4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------------+--------------------+\n",
      "|gender|age|                 uid|           user_json|\n",
      "+------+---+--------------------+--------------------+\n",
      "|     0|  0|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     1|  1|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "|     0|  1|d50237ea-747e-48a...|{\"visits\": [{\"url...|\n",
      "|     0|  1|d502f29f-d57a-46b...|{\"visits\": [{\"url...|\n",
      "|     1|  4|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|\n",
      "+------+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.replace(gender_dict, subset=[\"gender\"])\\\n",
    "                       .replace(age_dict, subset=[\"age\"])\\\n",
    "                       .withColumn('gender', F.col(\"gender\").cast(IntegerType()))\\\n",
    "                       .withColumn('age', F.col(\"age\").cast(IntegerType()))\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_data = train_data.select(\"uid\", \"age\")\n",
    "gender_data = train_data.select(\"uid\", \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_jsons = train_data.select(\"user_json\").collect()\n",
    "uids = train_data.select(\"uid\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('timestamp', StringType()),\\\n",
    "                    StructField('url', StringType()),\\\n",
    "                    StructField('uid', StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(uid, user_json):\n",
    "    data = json.loads(user_json[0])['visits']\n",
    "    df = spark.createDataFrame(Row(**x) for x in data)\n",
    "    df = df.withColumn('timestamp', F.col('timestamp').cast(LongType()))\n",
    "    df = df.withColumn('uid', F.lit(uid[0]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|    timestamp|                 url|                 uid|\n",
      "+-------------+--------------------+--------------------+\n",
      "|1419755762980|http://muz4in.net...|d51294ed-1b95-4e4...|\n",
      "|1419581810838|http://www.smachn...|d51294ed-1b95-4e4...|\n",
      "|1418632266469|http://www.fotoco...|d51294ed-1b95-4e4...|\n",
      "|1418632216018|http://fotocollag...|d51294ed-1b95-4e4...|\n",
      "|1418123008253|http://www.labiri...|d51294ed-1b95-4e4...|\n",
      "|1418123007649|http://www.labiri...|d51294ed-1b95-4e4...|\n",
      "|1417932238909|http://kaluga.tax...|d51294ed-1b95-4e4...|\n",
      "|1427126630000|http://love.mail....|d51294ed-1b95-4e4...|\n",
      "|1427126509000|http://love.mail....|d51294ed-1b95-4e4...|\n",
      "|1427126307000|http://go.youlame...|d51294ed-1b95-4e4...|\n",
      "|1427121385001|http://angelreiki...|d51294ed-1b95-4e4...|\n",
      "|1427121385000|http://google.ru/...|d51294ed-1b95-4e4...|\n",
      "|1427121331000|http://www.angelr...|d51294ed-1b95-4e4...|\n",
      "|1427121329001|http://www.angelr...|d51294ed-1b95-4e4...|\n",
      "|1427121329000|https://www.googl...|d51294ed-1b95-4e4...|\n",
      "|1427119320000|http://love.mail....|d51294ed-1b95-4e4...|\n",
      "|1427118293000|http://love.mail....|d51294ed-1b95-4e4...|\n",
      "|1427117735000|http://indiada.ru...|d51294ed-1b95-4e4...|\n",
      "|1427117472000|http://love.mail....|d51294ed-1b95-4e4...|\n",
      "|1427007127000|http://cache.betw...|d51294ed-1b95-4e4...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydf = create_df(uids[10], user_jsons[10])\n",
    "mydf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df = spark.createDataFrame([], schema).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(uids)), leave=True):\n",
    "    df = create_df(uids[i], user_jsons[i]).toPandas()\n",
    "    view_df = view_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df.to_csv('view_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df.write.csv(\"view_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_view = StructType(fields=[StructField(\"timestamp\", StringType()),\n",
    "                                StructField(\"url\", StringType()),\n",
    "                                StructField(\"uid\", StringType())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|    timestamp|                 url|                 uid|\n",
      "+-------------+--------------------+--------------------+\n",
      "|1419688144068|http://zebra-zoya...|d50192e5-c44e-4ae...|\n",
      "|1426666298001|http://news.yande...|d50192e5-c44e-4ae...|\n",
      "|1426666298000|http://www.sotovi...|d50192e5-c44e-4ae...|\n",
      "|1426661722001|http://news.yande...|d50192e5-c44e-4ae...|\n",
      "|1426661722000|http://www.sotovi...|d50192e5-c44e-4ae...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_df = spark.read.csv('view_df.csv', schema=schema_view)\n",
    "view_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_df.filter(view_df.url.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5314400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5312174"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_df = view_df.na.drop(\"any\")\n",
    "view_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import DenseVector, Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "def tokenizing(string):\n",
    "    return regex.findall(string.lower())\n",
    "\n",
    "def sparse_to_array(v):\n",
    "    v = Vectors.dense(v)\n",
    "    new_array = list([float(x) for x in v])\n",
    "    return new_array\n",
    "\n",
    "tokenization = F.udf(tokenizing, ArrayType(StringType()))\n",
    "convert_dense = F.udf(lambda vector: Vectors.dense(vector), VectorUDT())\n",
    "#convert_dense = F.udf(sparse_to_array, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(numFeatures=750, inputCol=\"tokens\", outputCol=\"tf\")\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " uid        | 0392f398-ea7e-4a1... \n",
      " url        | http://www.adme.r... \n",
      " count_time | 212                  \n",
      "-RECORD 1--------------------------\n",
      " uid        | 094b1e7e-97a6-441... \n",
      " url        | http://kinoclips.... \n",
      " count_time | 62                   \n",
      "-RECORD 2--------------------------\n",
      " uid        | 095544a2-64f7-422... \n",
      " url        | http://well-good.... \n",
      " count_time | 4                    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_df = view_df.groupBy(\"uid\")\\\n",
    "                        .agg(F.concat_ws(\", \", F.collect_list(view_df.url)).alias(\"url\"),\\\n",
    "                             F.count(\"timestamp\").alias(\"count_time\"))\n",
    "view_df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " uid        | 0392f398-ea7e-4a1... \n",
      " url        | http://www.adme.r... \n",
      " count_time | 212                  \n",
      " tokens     | [http, www, adme,... \n",
      " tf         | (750,[0,7,8,10,12... \n",
      " features   | [9.36918946316104... \n",
      "-RECORD 1--------------------------\n",
      " uid        | 094b1e7e-97a6-441... \n",
      " url        | http://kinoclips.... \n",
      " count_time | 62                   \n",
      " tokens     | [http, kinoclips,... \n",
      " tf         | (750,[0,8,10,11,1... \n",
      " features   | [11.2430273557932... \n",
      "-RECORD 2--------------------------\n",
      " uid        | 095544a2-64f7-422... \n",
      " url        | http://well-good.... \n",
      " count_time | 4                    \n",
      " tokens     | [http, well, good... \n",
      " tf         | (750,[147,165,407... \n",
      " features   | [0.0,0.0,0.0,0.0,... \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_df = view_df.withColumn(\"tokens\", tokenization(\"url\"))\n",
    "view_df = hashingTF.transform(view_df)\n",
    "idf_model_view = idf.fit(view_df)\n",
    "view_df = idf_model_view.transform(view_df)\n",
    "view_df = view_df.withColumn(\"features\", convert_dense(\"features\"))\n",
    "view_df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=[\"features\", \"count_time\"], outputCol=\"result_features\")\n",
    "view_df = vec_assembler.transform(view_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|            features|age|\n",
      "+--------------------+---+\n",
      "|[9.36918946316104...|  4|\n",
      "|[11.2430273557932...|  0|\n",
      "|[0.0,0.0,0.0,0.0,...|  1|\n",
      "|[0.0,0.0,0.0,2.02...|  4|\n",
      "|[0.0,0.0,0.0,0.0,...|  4|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_data = age_data.join(view_df, on=\"uid\", how=\"inner\")\n",
    "age_data = age_data.select(\"features\", \"age\")\n",
    "age_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36138"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|gender|\n",
      "+--------------------+------+\n",
      "|[9.36918946316104...|     0|\n",
      "|[11.2430273557932...|     0|\n",
      "|[0.0,0.0,0.0,0.0,...|     0|\n",
      "|[0.0,0.0,0.0,2.02...|     0|\n",
      "|[0.0,0.0,0.0,0.0,...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_data = gender_data.join(view_df, on=\"uid\", how=\"inner\")\n",
    "gender_data = gender_data.select(\"features\", \"gender\")\n",
    "gender_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36138"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('result_features', 'vector'), ('gender', 'int')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gender=1, count=18698), Row(gender=0, count=17440)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_data.groupby(\"gender\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBTClassifier для пола"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_gender = GBTClassifier(featuresCol='features', labelCol='gender', maxDepth=3, maxIter=25, stepSize=0.1)\n",
    "evaluator_gender = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"gender\", metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6041562897780729"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gbt_gender = gbt_gender.fit(gender_data)\n",
    "predictions_train_gender = model_gbt_gender.transform(gender_data)\n",
    "evaluator_gender.evaluate(predictions_train_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_model_view.save('idf_model_750')\n",
    "model_gbt_gender.save('gbt_gender_750')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier для возраста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_age = RandomForestClassifier(featuresCol='features', labelCol='age', maxDepth=7, numTrees=30)\n",
    "evaluator_age = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"age\", metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.460125076097183"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf_age = rf_age.fit(age_data)\n",
    "predictions_train_age = model_rf_age.transform(age_data)\n",
    "evaluator_age.evaluate(predictions_train_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_age.save('rf_age_750')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_kafka = StructType(fields=[StructField(\"uid\", StringType()),\n",
    "                                StructField(\"visits\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(string):\n",
    "    data = json.loads(string)\n",
    "    s =''\n",
    "    for i in data:\n",
    "        s = ' '.join([s, i['url']])\n",
    "    return str(s)\n",
    "parsing = F.udf(parse_json, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_gender_dict = {'0': 'F', '1': 'M'}\n",
    "reverse_age_dict = {'0': '18-24', '1': '25-34', '2': '35-44', '3': '45-54', '4': '>=55'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_age_load = RandomForestClassificationModel.load('rf_age_750')\n",
    "gbt_gender_load = GBTClassificationModel.load('gbt_gender_750')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_kafka(topic, data):\n",
    "    write_kafka_params = {\"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667'}\n",
    "    kafka_doc = to_json(struct(col(\"*\")))\n",
    "    data.select(kafka_doc.alias(\"value\"))\\\n",
    "        .withColumn(\"topic\", lit(topic))\\\n",
    "        .write.format(\"kafka\")\\\n",
    "        .options(**write_kafka_params).save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    #\"subscribe\": \"test_topic0, test_topic1, test_topic2\"\n",
    "    \"subscribe\": \"input_kirill.kuznetsov\",\n",
    "    #\"startingOffsets\": \"latest\"\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "kafka_df = kafka_sdf.select(F.col(\"value\").cast(\"string\"))\n",
    "kafka_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema_kafka).alias(\"s\")).select(col(\"s.*\"))\n",
    "kafka_df = kafka_df.withColumn('url', parsing('visits')).select('uid', 'url')\n",
    "kafka_df = kafka_df.withColumn(\"tokens\", tokenization(\"url\"))\n",
    "kafka_df = hashingTF.transform(kafka_df)\n",
    "kafka_df = idf_model_view.transform(kafka_df)\n",
    "kafka_df = kafka_df.withColumn(\"features\", convert_dense(\"features\"))\n",
    "kafka_df = rf_age_load.transform(kafka_df).withColumn('age', F.col('prediction')).select('uid', 'features', 'age')\n",
    "kafka_df = kafka_df.withColumn('age', F.col('age').cast(IntegerType()).cast(StringType()))\n",
    "kafka_df = kafka_df.replace(reverse_age_dict, subset=[\"age\"])\n",
    "kafka_df = gbt_gender_load.transform(kafka_df).withColumn('gender', F.col('prediction')).select('uid', 'gender', 'age')\n",
    "kafka_df = kafka_df.withColumn('gender', F.col('gender').cast(IntegerType()).cast(StringType()))\n",
    "kafka_df = kafka_df.replace(reverse_gender_dict, subset=[\"gender\"])\n",
    "\n",
    "kafka_doc = to_json(struct(col(\"*\")))\n",
    "kafka_df = kafka_df.select(kafka_doc.alias('value'))\n",
    "\n",
    "#kafka_df = kafka_df.withColumn('value', convert2str('uid', 'gender', 'age')).select('value')\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-node-1.newprolab.com:6667',\n",
    "   \"topic\": \"kirill.kuznetsov\"\n",
    "}\n",
    "\n",
    "sink = kafka_df.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-03488fa6c21e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mkafka_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mkafka_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkafka_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkafka_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mkafka_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#kafka_df = kafka_df.withColumn('value', col('age'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#kafka_df = kafka_df.withColumn('res', convert2str('uid', 'gender', 'age'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    #\"subscribe\": \"test_topic0, test_topic1, test_topic2\"\n",
    "    \"subscribe\": \"input_kirill.kuznetsov\",\n",
    "    #\"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "kafka_sdf = spark.read.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "kafka_df = kafka_sdf.select(F.col(\"value\").cast(\"string\"))\n",
    "kafka_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema_kafka).alias(\"s\")).select(col(\"s.*\"))\n",
    "kafka_df = kafka_df.withColumn('url', parsing('visits')).select('uid', 'url')\n",
    "kafka_df = kafka_df.withColumn(\"tokens\", tokenization(\"url\"))\n",
    "kafka_df = hashingTF.transform(kafka_df)\n",
    "kafka_df = idf_model_view.transform(kafka_df)\n",
    "kafka_df = kafka_df.withColumn(\"features\", convert_dense(\"features\"))\n",
    "kafka_df = rf_age_load.transform(kafka_df).withColumn('age', F.col('prediction')).select('uid', 'features', 'age')\n",
    "kafka_df = kafka_df.withColumn('age', F.col('age').cast(IntegerType()).cast(StringType()))\n",
    "kafka_df = kafka_df.replace(reverse_age_dict, subset=[\"age\"])\n",
    "kafka_df = gbt_gender_load.transform(kafka_df).withColumn('gender', F.col('prediction')).select('uid', 'gender', 'age')\n",
    "kafka_df = kafka_df.withColumn('gender', F.col('gender').cast(IntegerType()).cast(StringType()))\n",
    "kafka_df = kafka_df.replace(reverse_gender_dict, subset=[\"gender\"])\n",
    "kafka_doc = to_json(struct(col(\"*\")))\n",
    "kafka_df = kafka_df.select(kafka_doc.alias('value'))\n",
    "kafka_df.show(5000)\n",
    "#kafka_df = kafka_df.withColumn('value', col('age'))\n",
    "#kafka_df = kafka_df.withColumn('res', convert2str('uid', 'gender', 'age'))\n",
    "write_kafka(\"kirill.kuznetsov\", kafka_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ae35e7a5c51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sq' is not defined"
     ]
    }
   ],
   "source": [
    "sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
